{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8386f02f-85ba-4559-8139-dbb86d2b9d74",
   "metadata": {},
   "source": [
    "Install required frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89319a1e-7cfb-4bac-a159-ae74d7996d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: llama-index in /opt/conda/lib/python3.11/site-packages (0.11.7)\n",
      "Requirement already satisfied: llama-index-llms-ollama in /opt/conda/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /opt/conda/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.11.7)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.4)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: ollama>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-llms-ollama) (0.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.5)\n",
      "Requirement already satisfied: minijinja>=1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.44.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.4.49)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (3.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (2.7.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /opt/conda/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.0.17)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (2024.7.24)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence_transformers) (70.1.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence_transformers) (0.43.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.19)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence_transformers llama-index llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab8d3f-2ad9-4455-a900-e449a850ca3e",
   "metadata": {},
   "source": [
    "Just in case you are a sudoer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdce04c-ac6a-4634-b4ea-3be645422518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: The \"no new privileges\" flag is set, which prevents sudo from running as root.\n",
      "sudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.\n",
      "/bin/bash: line 1: ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "!sudo curl -fsSL https://ollama.com/install.sh | sh\n",
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "998e0a17-390f-4ed4-a031-0f0abf3a9f9c",
   "metadata": {},
   "source": [
    "You probably are not a sudoer, and you cannot run processes in background. So, execute the following commands in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb1dd2-64ec-4811-aa0c-eb739703bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-07 07:58:18--  https://github.com/ollama/ollama/releases/download/v0.3.9/ollama-linux-amd64.tgz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/658928958/834b7c1e-8b7b-49fc-9e79-8ee9dbbe4d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240907%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240907T055818Z&X-Amz-Expires=300&X-Amz-Signature=d71b4de224cf44fe1c874a89fd0cc4d9c3fd2c531e12ee427acb0fbd958b8d4c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=658928958&response-content-disposition=attachment%3B%20filename%3Dollama-linux-amd64.tgz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-09-07 07:58:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/658928958/834b7c1e-8b7b-49fc-9e79-8ee9dbbe4d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240907%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240907T055818Z&X-Amz-Expires=300&X-Amz-Signature=d71b4de224cf44fe1c874a89fd0cc4d9c3fd2c531e12ee427acb0fbd958b8d4c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=658928958&response-content-disposition=attachment%3B%20filename%3Dollama-linux-amd64.tgz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1495394106 (1.4G) [application/octet-stream]\n",
      "Saving to: ‘ollama-linux-amd64.tgz.2’\n",
      "\n",
      "ollama-linux-amd64. 100%[===================>]   1.39G   314MB/s    in 4.5s    \n",
      "\n",
      "2024-09-07 07:58:23 (320 MB/s) - ‘ollama-linux-amd64.tgz.2’ saved [1495394106/1495394106]\n",
      "\n",
      "./\n",
      "./lib/\n",
      "./lib/ollama/\n",
      "./lib/ollama/libcublas.so.12.4.2.65\n",
      "./lib/ollama/libcublasLt.so.11\n",
      "./lib/ollama/libcublas.so.11.5.1.109\n",
      "./lib/ollama/libcudart.so.11.3.109\n",
      "./lib/ollama/libcublas.so.12\n",
      "./lib/ollama/libcublasLt.so\n",
      "./lib/ollama/libcublas.so.11\n",
      "./lib/ollama/libcublas.so\n",
      "./lib/ollama/libcudart.so\n",
      "./lib/ollama/libcublasLt.so.12\n",
      "./lib/ollama/libcublasLt.so.11.5.1.109\n",
      "./lib/ollama/libcudart.so.11.0\n",
      "./lib/ollama/libcudart.so.12.4.99\n",
      "./lib/ollama/libcudart.so.12\n",
      "./lib/ollama/libcublasLt.so.12.4.2.65\n",
      "./bin/\n",
      "./bin/ollama\n",
      "2024/09/07 07:58:40 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/pierre.jourlin/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
      "time=2024-09-07T07:58:40.298+02:00 level=INFO source=images.go:753 msg=\"total blobs: 5\"\n",
      "time=2024-09-07T07:58:40.298+02:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-09-07T07:58:40.299+02:00 level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.9)\"\n",
      "time=2024-09-07T07:58:40.299+02:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama4020088056/runners\n",
      "time=2024-09-07T07:58:52.624+02:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60102]\"\n",
      "time=2024-09-07T07:58:52.624+02:00 level=INFO source=gpu.go:200 msg=\"looking for compatible GPUs\"\n",
      "time=2024-09-07T07:58:52.777+02:00 level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-63f6c305-f707-9f54-f682-51bfbba8bc3e library=cuda variant=v12 compute=7.0 driver=12.2 name=\"Tesla V100-SXM2-16GB\" total=\"15.8 GiB\" available=\"15.5 GiB\"\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ollama/ollama/releases/download/v0.3.9/ollama-linux-amd64.tgz\n",
    "!tar xvfz ollama-linux-amd64.tgz\n",
    "!./bin/ollama serve &\n",
    "!./bin/ollama pull llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed17595-c3a5-4755-ad60-14d6aadbc573",
   "metadata": {},
   "source": [
    "# Prepare the patent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df23ac3-af64-428a-b7c2-b4ffcf15ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 491330 publications publications containing the all following terms: ['medical', 'device']\n",
      "Storing 500 patents to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:12<00:00, 41.11it/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!rm data/*\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PROD : >7M publications\n",
    "# TEST : ~10K publications\n",
    "epab = EPABClient(env=\"PROD\")\n",
    "# q = epab.query_inventor_name(\"JOURLIN, Pierre\")\n",
    "terms = [\"medical\", \"device\"]\n",
    "q = epab.query_description(text=\",\".join(terms), match_all=True, ignore_case=True)\n",
    "print(f\"Found {q} publications containing the all following terms: {terms}\")\n",
    "limit = 500\n",
    "tab = q.get_results(\n",
    "    \"epab_doc_id, title.fr, abstract, description, publication, inventor\", limit=limit\n",
    ")\n",
    "print(f\"Storing {limit} patents to disk...\")\n",
    "for offset in tqdm(range(limit)):\n",
    "    data = tab[\"description.text\"][offset]\n",
    "    with open(\"data/\" + tab[\"epab_doc_id\"][offset] + \".txt\", \"w\") as file:\n",
    "        print(data, file=file)\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    # print(soup.get_text())\n",
    "#    root = ET.fromstring(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02796d-75d9-485a-96c0-c91c12230bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing patents can take some time...\n",
      "Indexing patents completed...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How can I help ? (answer 'bye' to quit) \n",
      "> What is the best device in medecine ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, it is difficult to determine what the \"best\" device in medicine is. The text describes various medical devices, including bandages, stethoscopes, examination gloves, colostomy bags, oxygen masks, thermometers, blood transfusion tubes, catheters, balloon catheters, prosthetic heart valves, and pacemakers.\n"
     ]
    }
   ],
   "source": [
    "from epo.tipdata.epab import EPABClient\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"NeuML/pubmedbert-base-embeddings-matryoshka\"\n",
    ")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "print(\"Indexing patents can take some time...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "print(\"Indexing patents completed...\")\n",
    "query_engine = index.as_query_engine()\n",
    "while True:\n",
    "    query = input(\"How can I help ? (answer 'bye' to quit) \" + \"\\n>\")\n",
    "    if query == \"bye\":\n",
    "        break\n",
    "    response = query_engine.query(query)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

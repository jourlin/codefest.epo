<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>toolkit API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>toolkit</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="toolkit.Toolkit"><code class="flex name class">
<span>class <span class="ident">Toolkit</span></span>
<span>(</span><span>read_only=False, index_name='BOTH')</span>
</code></dt>
<dd>
<div class="desc"><p>Toolkit is the main structure for handling HF embeddings,
Ollama, Deeplake &amp; LlamaIndex
Note : Docstrings are formatted for pdoc3</p>
<p>Initialise a Toolkit object</p>
<h2 id="parameters">Parameters</h2>
<pre><code>read_only : bool
    Should we open the indexes only for search or for creating and populating
index_name : str
    "EP" stands for full-text patents, "UMLS" stands for UMLS entities, "BOTH" stands for both indexes
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>Toolkit
    A Toolkit object
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Toolkit:
    &#34;&#34;&#34;
    Toolkit is the main structure for handling HF embeddings, 
    Ollama, Deeplake &amp; LlamaIndex
    Note : Docstrings are formatted for pdoc3
    &#34;&#34;&#34;
    def __init__(self, read_only=False, index_name=&#34;BOTH&#34;):
        &#34;&#34;&#34;
        Initialise a Toolkit object

        Parameters
        ----------
            read_only : bool
                Should we open the indexes only for search or for creating and populating
            index_name : str
                &#34;EP&#34; stands for full-text patents, &#34;UMLS&#34; stands for UMLS entities, &#34;BOTH&#34; stands for both indexes
        Returns
        -------
            Toolkit
                A Toolkit object
        &#34;&#34;&#34;
        # import all configuration values from .env file
        self.query=os.getenv(&#39;SEARCH_TERM&#39;)
        self.model_name=os.getenv(&#39;MODEL_NAME&#39;)
        self.match_all=os.getenv(&#39;MATCH_ALL&#39;)
        self.ignore_case=os.getenv(&#39;IGNORE_CASE&#39;)
        self.epab_size=os.getenv(&#39;EPAB_SIZE&#39;)
        self.doc_limit=int(os.getenv(&#39;DOC_LIMIT&#39;))
        self.llm=os.getenv(&#39;LLM&#39;)
        self.llm_req_timeout=float(os.getenv(&#39;LLM_REQ_TIMEOUT&#39;))
        self.token_limit=int(os.getenv(&#39;TOKEN_LIMIT&#39;))
        self.document_dir=os.getenv(&#39;DOC_DIR&#39;)
        self.vector_dir=os.getenv(&#39;VEC_DIR&#39;)
        self.umls_document_dir=os.getenv(&#39;UMLS_DOC_DIR&#39;)
        self.umls_vector_dir=os.getenv(&#39;UMLS_VEC_DIR&#39;)
        self.tmp_dir=os.getenv(&#39;TMP_DIR&#39;)
        self.table_cells_maxchars=int(os.getenv(&#39;TABLE_CELLS_MAXCHARS&#39;))
        self.span_top_k = int(os.getenv(&#39;SPAN_TOPK&#39;))
        print(&#34;Initializing toolkit...&#34;,file=sys.stderr)
        # Get embedding model (used to build vectors from queries and document spans) from HF
        self.embed_model = HuggingFaceEmbedding(
            model_name=self.model_name
        )
        Settings.embed_model = self.embed_model
        # Tell LlamaIndex to call Ollama for interacting with LLM (e.g. Llama3)
        self.llm_settings = Ollama(model=self.llm, request_timeout=self.llm_req_timeout)
        Settings.llm = self.llm_settings
        accepted_names = [&#34;UMLS&#34;, &#34;EP&#34;, &#34;BOTH&#34;]
        # Select one of the available index
        # Check index name validity
        if index_name not in accepted_names:
            print(f&#34;Error: &#39;{index_name}&#39; is not a valid index name. Accepted values : {accepted_names}&#34;, file=sys.stderr)
            sys.exit(-1)
        # Select UMLS entities
        if index_name==&#34;UMLS&#34; or index_name==&#34;BOTH&#34;:
            # Configure deeplake for UMLS
            self.umls_vector_store = DeepLakeVectorStore(dataset_path=self.umls_vector_dir)
            self.umls_index = VectorStoreIndex.from_vector_store(vector_store=self.umls_vector_store, streaming=True, read_only=read_only)
            self.umls_storage_context = StorageContext.from_defaults(vector_store=self.umls_vector_store)
        # Select EPO full-text patents
        if index_name==&#34;EP&#34; or index_name==&#34;BOTH&#34;: 
            # Configure deeplake for patents
            self.vector_store = DeepLakeVectorStore(dataset_path=self.vector_dir)
            self.index = VectorStoreIndex.from_vector_store(vector_store=self.vector_store, streaming=True, read_only=read_only)
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            self.memory = ChatMemoryBuffer.from_defaults(token_limit=self.token_limit)
            # Here is the prompt :
            self.chat_engine = self.index.as_chat_engine(
            chat_mode=&#34;context&#34;,
            memory=self.memory,
            system_prompt=(
                &#34;You are a chatbot, able to have normal interactions, as well as talk&#34;
                &#34; about patents. Do not invent patent numbers.&#34;
            ),
        )
        # If needed, makes data directories
        os.system(&#34;mkdir -p &#34;+self.document_dir)
        os.system(&#34;mkdir -p &#34;+self.vector_dir)
        os.system(&#34;mkdir -p &#34;+self.umls_document_dir)
        os.system(&#34;mkdir -p &#34;+self.umls_vector_dir)
        print(&#34;Initialization completed...&#34;,file=sys.stderr)

    def get_ai_generated_field(self,text, field):
        &#34;&#34;&#34;
        Extract a brief description of major strengths of the invention

        Parameters
        ----------
            text : str
                The text to be processed
            field : str
                output&#39;s field name (e.g. &#34;strength&#34;, see ai_generated_prompts global variable)
        Returns
        -------
            str
                the generated text
        &#34;&#34;&#34;
        llm = Ollama(model=self.llm)
        messages=[
            ChatMessage(role=&#34;assistant&#34;, content=&#34;You are an assistant, do what the user tells you to do properly.&#34;),
            ChatMessage(role=&#34;user&#34;, content=ai_generated_prompts[field][1]+text)
        ]
        # start llm inference
        resp = llm.chat(messages)
        content = None
        # parse the LLM output
        for item in resp:
            if isinstance(item, tuple) and item[0] == &#39;message&#39;:
                content = item[1].content
                break
        # return LLM output
        return content
    
    def retrieve(self, query, query_is_file=False):
        &#34;&#34;&#34;
        Retrieve patents by performing a K Nearest Neighbours,
        based on query and patents embeddings 

        Parameters
        ----------
            query : str
                input text for query search or document similarity search
            query_is_file : bool
                Toggle between query search / similarity search
        Returns
        -------
            str
                a string containing the ranked list 
                of retrieved documents as a HTML table 
        &#34;&#34;&#34;
        if not query_is_file:
            # First expand all UMLS concepts contained in the query
            query = self.expand_query(query)
        # LLama Index does not provide the search() method for its embedded Deeplake stores, so : 
        store = deeplake.core.vectorstore.deeplake_vectorstore.DeepLakeVectorStore(path=self.vector_dir)
        result = store.search(embedding_data=query, embedding_function=self.embed_model.get_text_embedding, k=self.span_top_k)
        # Get retrieved filenames from Deeplake results
        docname_list ={result[&#39;metadata&#39;][offset][&#39;file_path&#39;] for offset in range(0, len(result[&#39;metadata&#39;]))}
        # Render results as a HTML table
        output=&#34;&lt;table&gt;&lt;tr&gt;&lt;th&gt;select&lt;/th&gt;&#34;
        # Column names for XML fields
        for field in xml_extraction_data:
            if &#34;display&#34; in field:
                output+=f&#34;&lt;th&gt;{field[&#39;display&#39;]}&lt;/th&gt;&#34;
        # Column names for AI generated fields    
        for field in ai_generated_prompts.keys():
            output+=f&#34;&lt;th&gt;âœ¨{ai_generated_prompts[field][0]} (AI generated)&lt;/th&gt;&#34;
        output+=&#34;&lt;/tr&gt;\n&#34;
        # Parse all retrieved XML document to extract relevant information
        for doc in docname_list:
            try:
                root=ET.parse(doc).getroot()
            except:
                continue
            # extract XML fields values
            patent_id=&#34;?&#34;
            application_id=&#34;?&#34;
            for field in xml_extraction_data:
                # Value is a parameter value within the Nth result of a xpath query
                if &#34;position&#34; in field and &#34;parameter&#34; in field:
                    field[&#34;value&#34;]=root.xpath(field[&#34;path&#34;])[field[&#34;position&#34;]].get(field[&#34;parameter&#34;])
                # Value is the Nth result of a xpath query
                elif &#34;position&#34; in field:
                    field[&#34;value&#34;]=root.xpath(field[&#34;path&#34;])[field[&#34;position&#34;]]
                if field[&#34;name&#34;]==&#34;date&#34; and date_pattern.match(field[&#34;value&#34;]): # Format &#34;20241018&#34; dates as &#34;18/10/2024&#34;
                    year=field[&#34;value&#34;][:4]
                    month=field[&#34;value&#34;][4:6]
                    day=field[&#34;value&#34;][6:8]
                    field[&#34;value&#34;]=day+&#39;/&#39;+month+&#34;/&#34;+year
                # Build patent id as country+doc_number+kind
                if field[&#34;name&#34;]==&#34;country&#34;:
                    patent_id=field[&#34;value&#34;]
                if field[&#34;name&#34;] == &#34;doc-number&#34;:
                    patent_id+=field[&#34;value&#34;]
                    doc_number=field[&#34;value&#34;]
                if field[&#34;name&#34;] == &#34;kind&#34;:
                    patent_id+=field[&#34;value&#34;]                    
                if field[&#34;name&#34;]==&#34;patent_id&#34;:
                    field[&#34;value&#34;]=patent_id
                if field[&#34;name&#34;]==&#34;id&#34;:
                    application_id=field[&#34;value&#34;]
            output+=&#39;&lt;tr&gt;&#39;
            output+=&#34;&lt;td&gt;&#34;+&#39;&lt;input type=&#34;checkbox&#34; id=&#34;&#39;+application_id+&#39;&#34; onchange=&#34;append_query(this)&#34; &gt;&lt;/td&gt;&#39;
            output+=&#39;&lt;td&gt;&lt;a href=&#34;/download/&#39;+os.path.basename(doc).strip(&#34;.xml&#34;)+&#39;.pdf&#34;&#39;
            # output of XML fields
            for field in xml_extraction_data:
                if field[&#34;name&#34;]==&#34;id&#34;:
                    output+=f&#34;&gt;{field[&#34;value&#34;]}&lt;/a&gt;&lt;/td&gt;&#34;
                elif &#34;display&#34; in field:
                    output+=f&#34;&lt;td&gt;{field[&#34;value&#34;]}&lt;/td&gt;&#34;
            # output of AI generated fields
            for field in ai_generated_prompts.keys():
                with open(doc.strip(&#34;.xml&#34;)+&#34;.&#34;+field+&#39;.html&#39;, &#34;r&#34;) as file:
                    content = file.read()
                    output+=f&#34;&lt;td&gt;{content}&lt;/td&gt;&#34;
            output+=&#34;&lt;/tr&gt;\n&#34;
        output+=&#34;&lt;/table&gt;\n&#34;
        return output

    def extend(self, query):
        &#34;&#34;&#34;
        Retrieve UMLS entities by performing a K Nearest Neighbours, 
        based on query and UMLS embeddings 
        
        Parameters
        ----------
            query : str
                query text to be expanded
        Returns
        -------
            str
                a string that contains an ordered list of 
                retrieved UMLS concepts as a HTML table
        &#34;&#34;&#34;
        store = deeplake.core.vectorstore.deeplake_vectorstore.DeepLakeVectorStore(path=self.umls_vector_dir)
        result = store.search(embedding_data=query, embedding_function=self.embed_model.get_text_embedding, k=self.span_top_k)
        # Get retrieved concept IDs and their contents
        concept_list = [result[&#39;metadata&#39;][offset][&#39;file_name&#39;] for offset in range(0, len(result[&#39;metadata&#39;]))]
        content_list = [result[&#39;text&#39;][offset] for offset in range(0, len(result[&#39;text&#39;]))]
        output=&#34;&lt;table&gt;&lt;tr&gt;&lt;th&gt;select&lt;/th&gt;&lt;th&gt;concept&lt;/th&gt;&lt;th&gt;alternatives&lt;/th&gt;&lt;/tr&gt;\n&#34;
        num_lines=0
        for offset in range(0, len(result[&#39;text&#39;])):
            num_lines+=1
            # As some concepts might be empty, we need to retrieve more than needed,
            # So stop once enough 
            if num_lines &gt; int(os.getenv(&#39;MAXNUM_DISPLAYED_CONCEPTS&#39;))+1:
                break
            # Don&#39;t display long descriptions
            forms= [html2text(x) for x in content_list[offset].split(&#34;\n&#34;) if x !=&#34;&#34; and len(x)&lt;int(os.getenv(&#39;MAX_LEN_FOR_CONCEPT_DESC&#39;))]
            # Don&#39;t display spurious forms
            forms = sorted(list(filter(lambda f: len(f)&gt;3, forms)))
            if len(forms)&lt;=0: # skip if concept has no form
                continue
            # HTML for the row of selectable concept
            concept_id = concept_list[offset].strip(&#34;.txt&#34;)
            output+=&#34;&lt;tr&gt;&#34;
            output+=&#34;&lt;td&gt;&#34;+&#39;&lt;input type=&#34;checkbox&#34; id=&#34;&#39;+concept_id+&#39;&#34; onchange=&#34;append_query(this)&#34; &gt;&lt;/td&gt;&#39;
            output+=&#39;&lt;td&gt;&lt;b&gt;&#39;+forms[0]+&#34;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&#34;+&#34; ; &#34;.join(forms)+&#34;&lt;/td&gt;&#34;
            output+=&#34;&lt;/tr&gt;&#34;
        output+=&#34;&lt;/table&gt;\n&#34;
        return output
    
    def filter_query(self, query):
        &#34;&#34;&#34;
        remove concept IDs from query 

        Parameters
        ----------
            query : str
                input query text that might contain UMLS entities IDs
        Returns
        -------
            str
                the filtered query
        &#34;&#34;&#34;
        terms=query.split()
        filtered = list(filter(lambda t: not concept_pattern.match(t), terms))
        return &#34; &#34;.join(filtered)
    
    def expand_query(self, query):
        &#34;&#34;&#34;
        Replace concept IDs in query by their contents

        Parameters
        ----------
            query : str
                unexpanded query text
        Returns
        -------
            str
                the expanded query text
        &#34;&#34;&#34;
        terms=query.split()
        # get UMLS concepts from query
        concepts = list(filter(lambda t: concept_pattern.match(t), terms))
        # remove UMLS concepts from query
        filtered = list(filter(lambda t: not concept_pattern.match(t), terms))
        query = &#34; &#34;.join(filtered)
        # do query expansion
        for concept in concepts:
            with open(self.umls_document_dir+&#34;/&#34;+concept[:4]+&#34;/&#34;+concept+&#34;.txt&#34;) as f:
                content = f.readlines()
            query+=&#34; &#34;+html2text(&#34; &#34;.join(content)).replace(&#39;\n&#39;, &#39; &#39;)
        return query

    def reindex(self, index_name):
        &#34;&#34;&#34;
        Load, store, index data as vectors of text spans embedding

        Parameters
        ----------
            index_name : str
                &#34;EP&#34; stands for full-text patents, &#34;UMLS&#34; stands for UMLS entities, &#34;BOTH&#34; stands for both indexes
        Returns
        -------
            None
                nothing
        &#34;&#34;&#34;
        # Indexing patents
        if index_name in [&#34;EP&#34;, &#34;BOTH&#34;]:
            print(f&#34;Reindexing &#39;{self.query}&#39;...&#34;)
            # Find in all files those that contains the filter keywords
            file_list = os.popen(&#34;grep -i -l &#39;&#34;+self.query+&#34;&#39; &#34;+self.document_dir+&#34;/*.xml&#34;).read()
            file_list = file_list.splitlines()
            print(f&#34;Found {len(file_list)} publications containing the following term: &#39;{self.query}&#39;&#34;)
            nb_docs = min(self.doc_limit, len(file_list))
            print(f&#34;Storing {nb_docs} patents embeddings to disk...&#34;)
            # Clean up temporary document directory
            os.system(&#34;rm -fr &#34;+self.tmp_dir)
            os.system(&#34;mkdir -p &#34;+self.tmp_dir)
            # Copy all files that contain search keywords to temporary document directory
            for fn in tqdm(file_list):
                if len(fn)&gt;0:
                    os.system(&#34;cp &#34;+fn+&#34; &#34;+self.tmp_dir)
            # Load documents and build index
            print(&#34;loading data...&#34;)
            documents = SimpleDirectoryReader(self.tmp_dir).load_data(num_workers=int(os.getenv(&#39;NUM_WORKERS&#39;)))
            print(&#34;Extract AI generated fields...&#34;)
            for doc in tqdm(documents):
                # parse xml file
                root=ET.fromstring(bytes(doc.text, encoding=&#39;utf8&#39;))
                # Build patent ID as country+doc_number+kind and append as a tag to document
                pd=root.xpath(&#34;//ep-patent-document&#34;)[0]
                doc.metadata[&#34;patent id&#34;]=pd.get(&#34;country&#34;)+pd.get(&#34;doc-number&#34;)+pd.get(&#34;kind&#34;)
                # Extract AI generated files
                for field in ai_generated_prompts.keys():
                    filename = doc.metadata[&#34;file_path&#34;].strip(&#34;.xml&#34;)+&#39;.&#39;+field+&#39;.html&#39;
                    with open(filename, &#34;w&#34;) as file:
                        file.write(markdown(self.get_ai_generated_field(&#34;\n&#34;.join(root.xpath(&#39;//text()&#39;))[:self.token_limit], field)))
            # embedding model
            Settings.embed_model = self.embed_model
            # ollama
            Settings.llm = self.llm_settings
            print(&#34;Indexing patents can take some time...&#34;)
            self.index = VectorStoreIndex.from_documents(
                documents, show_progress=True, storage_context=self.storage_context
            )
            print(&#34;Indexing patents completed...&#34;)
        # Indexing UMLS concepts and their forms
        if index_name in [&#34;UMLS&#34;, &#34;BOTH&#34;]:
            print(f&#34;Reindexing UMLS...&#34;)
            os.system(&#34;mkdir -p &#34;+self.umls_vector_dir)
            # Load documents and build index
            concepts = SimpleDirectoryReader(self.umls_document_dir, recursive=True).load_data(num_workers=int(os.getenv(&#39;NUM_WORKERS&#39;)))
            # embedding model
            Settings.embed_model = self.embed_model
            # ollama
            Settings.llm = self.llm_settings
            print(&#34;Indexing umls concepts can take some time...&#34;)
            self.umls_index = VectorStoreIndex.from_documents(
                concepts, show_progress=True, storage_context=self.umls_storage_context
            )
            print(&#34;Indexing umls concepts completed...&#34;)
        if index_name not in [&#34;EP&#34;, &#34;UMLS&#34;, &#34;BOTH&#34;]:
            print(&#34;Error : Invalid index name. Choose &#39;EP&#39; for patents or &#39;UMLS&#39; for concepts&#34;, file=sys.stderr)

    def patchat(self, question):
        &#34;&#34;&#34;
        Start the chatbot in streaming mode

        Parameters
        -----------
            question : str
                current prompt
        Returns
        -------
            StreamingAgentChatResponse
                a stream where token are directed as they are inferred
                the StreamingAgentChatResponse class is defined in LlamaIndex module
        &#34;&#34;&#34;
        # remove concept IDs as their are not helpfull here
        question = self.filter_query(question)
        print(f&#34;Answering &#39;{question}&#39;&#34;, file=sys.stderr)
        streaming_response = self.chat_engine.stream_chat(question)
        print(streaming_response.__class__.__name__)
        return streaming_response</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="toolkit.Toolkit.expand_query"><code class="name flex">
<span>def <span class="ident">expand_query</span></span>(<span>self, query)</span>
</code></dt>
<dd>
<div class="desc"><p>Replace concept IDs in query by their contents</p>
<h2 id="parameters">Parameters</h2>
<pre><code>query : str
    unexpanded query text
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>str
    the expanded query text
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, query)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve UMLS entities by performing a K Nearest Neighbours,
based on query and UMLS embeddings </p>
<h2 id="parameters">Parameters</h2>
<pre><code>query : str
    query text to be expanded
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>str
    a string that contains an ordered list of 
    retrieved UMLS concepts as a HTML table
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.filter_query"><code class="name flex">
<span>def <span class="ident">filter_query</span></span>(<span>self, query)</span>
</code></dt>
<dd>
<div class="desc"><p>remove concept IDs from query </p>
<h2 id="parameters">Parameters</h2>
<pre><code>query : str
    input query text that might contain UMLS entities IDs
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>str
    the filtered query
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.get_ai_generated_field"><code class="name flex">
<span>def <span class="ident">get_ai_generated_field</span></span>(<span>self, text, field)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract a brief description of major strengths of the invention</p>
<h2 id="parameters">Parameters</h2>
<pre><code>text : str
    The text to be processed
field : str
    output's field name (e.g. "strength", see ai_generated_prompts global variable)
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>str
    the generated text
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.patchat"><code class="name flex">
<span>def <span class="ident">patchat</span></span>(<span>self, question)</span>
</code></dt>
<dd>
<div class="desc"><p>Start the chatbot in streaming mode</p>
<h2 id="parameters">Parameters</h2>
<pre><code>question : str
    current prompt
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>StreamingAgentChatResponse
    a stream where token are directed as they are inferred
    the StreamingAgentChatResponse class is defined in LlamaIndex module
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.reindex"><code class="name flex">
<span>def <span class="ident">reindex</span></span>(<span>self, index_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Load, store, index data as vectors of text spans embedding</p>
<h2 id="parameters">Parameters</h2>
<pre><code>index_name : str
    "EP" stands for full-text patents, "UMLS" stands for UMLS entities, "BOTH" stands for both indexes
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>None
    nothing
</code></pre></div>
</dd>
<dt id="toolkit.Toolkit.retrieve"><code class="name flex">
<span>def <span class="ident">retrieve</span></span>(<span>self, query, query_is_file=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve patents by performing a K Nearest Neighbours,
based on query and patents embeddings </p>
<h2 id="parameters">Parameters</h2>
<pre><code>query : str
    input text for query search or document similarity search
query_is_file : bool
    Toggle between query search / similarity search
</code></pre>
<h2 id="returns">Returns</h2>
<pre><code>str
    a string containing the ranked list 
    of retrieved documents as a HTML table
</code></pre></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="toolkit.Toolkit" href="#toolkit.Toolkit">Toolkit</a></code></h4>
<ul class="">
<li><code><a title="toolkit.Toolkit.expand_query" href="#toolkit.Toolkit.expand_query">expand_query</a></code></li>
<li><code><a title="toolkit.Toolkit.extend" href="#toolkit.Toolkit.extend">extend</a></code></li>
<li><code><a title="toolkit.Toolkit.filter_query" href="#toolkit.Toolkit.filter_query">filter_query</a></code></li>
<li><code><a title="toolkit.Toolkit.get_ai_generated_field" href="#toolkit.Toolkit.get_ai_generated_field">get_ai_generated_field</a></code></li>
<li><code><a title="toolkit.Toolkit.patchat" href="#toolkit.Toolkit.patchat">patchat</a></code></li>
<li><code><a title="toolkit.Toolkit.reindex" href="#toolkit.Toolkit.reindex">reindex</a></code></li>
<li><code><a title="toolkit.Toolkit.retrieve" href="#toolkit.Toolkit.retrieve">retrieve</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
